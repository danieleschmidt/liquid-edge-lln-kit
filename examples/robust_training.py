#!/usr/bin/env python3
"""Production-ready training with comprehensive error handling."""

import jax
import jax.numpy as jnp
import optax
import numpy as np
from pathlib import Path
from liquid_edge import LiquidNN, LiquidConfig
from liquid_edge.profiling import EnergyProfiler, ProfilingConfig
from liquid_edge.error_handling import (
    RobustErrorHandler, validate_inputs, retry_with_backoff, graceful_degradation
)
from liquid_edge.monitoring import LiquidNetworkMonitor, PerformanceMetrics, CircuitBreaker

def generate_robust_data(num_samples: int = 1000):
    """Generate synthetic sensor data with noise and edge cases."""
    key = jax.random.PRNGKey(42)
    
    # Generate diverse sensor patterns
    t = np.linspace(0, 10, 50)
    data = []
    targets = []
    
    for i in range(num_samples):
        # Simulate realistic sensor noise and failures
        noise_level = 0.1 + 0.05 * np.random.random()  # Variable noise
        
        # Sometimes simulate sensor failures (10% of time)
        if np.random.random() < 0.1:
            # Sensor failure: some readings stuck or NaN
            imu_accel = np.full_like(t, np.nan if np.random.random() < 0.3 else 0.0)
            imu_gyro = jnp.sin(t) + noise_level * jax.random.normal(key, (len(t),))
        else:
            # Normal operation
            imu_accel = jnp.sin(t + i * 0.1) + noise_level * jax.random.normal(key, (len(t),))
            imu_gyro = jnp.cos(t + i * 0.1) + noise_level * jax.random.normal(key, (len(t),))
        
        proximity = jnp.exp(-((t - 5) ** 2)) + noise_level * jax.random.normal(key, (len(t),))
        light = jnp.maximum(0, jnp.sin(2 * t + i * 0.05)) + noise_level * jax.random.normal(key, (len(t),))
        
        # Handle NaN values gracefully
        sample = jnp.stack([
            jnp.nan_to_num(imu_accel, nan=0.0), 
            jnp.nan_to_num(imu_gyro, nan=0.0), 
            jnp.clip(proximity, -10, 10),  # Clip extreme values
            jnp.clip(light, 0, 5)
        ], axis=1)
        
        # Generate control targets with safety constraints
        linear_vel = jnp.clip(jnp.where(proximity[-1] < 0.3, 0.1, 0.5), 0.0, 1.0)
        angular_vel = jnp.clip(jnp.where(proximity[-1] < 0.3, jnp.sign(imu_gyro[-1]) * 0.8, 0.0), -2.0, 2.0)
        
        target = jnp.array([linear_vel, angular_vel])
        
        data.append(sample[-1])  # Take last timestep
        targets.append(target)
        
        key, _ = jax.random.split(key)
    
    return jnp.array(data), jnp.array(targets)

class RobustTrainer:
    """Production-ready trainer with comprehensive error handling."""
    
    def __init__(self, model: LiquidNN, config: LiquidConfig):
        self.model = model
        self.config = config
        self.optimizer = optax.adam(config.learning_rate)
        self.error_handler = RobustErrorHandler()
        self.monitor = LiquidNetworkMonitor(config.hidden_dim)
        self.circuit_breaker = CircuitBreaker(failure_threshold=5)
        
    @retry_with_backoff(max_retries=3, backoff_factor=2.0)
    def safe_train_step(self, state, batch):
        """Training step with comprehensive error handling."""
        try:
            # Validate inputs
            inputs, targets = batch
            validate_inputs(inputs, expected_shape=(None, self.config.input_dim))
            validate_inputs(targets, expected_shape=(None, self.config.output_dim))
            
            # Check for NaN/inf in inputs
            if not (jnp.all(jnp.isfinite(inputs)) and jnp.all(jnp.isfinite(targets))):
                raise ValueError("NaN or inf detected in training data")
            
            def loss_fn(params):
                with self.circuit_breaker:
                    outputs, hidden = self.model.apply(params, inputs, training=True)
                    
                    # Validate outputs
                    if not jnp.all(jnp.isfinite(outputs)):
                        raise ValueError("Model produced NaN/inf outputs")
                    
                    # Task loss with gradient clipping awareness
                    task_loss = jnp.mean((outputs - targets) ** 2)
                    
                    # Energy penalty
                    estimated_energy = self.model.energy_estimate(inputs.shape[0])
                    energy_penalty = jnp.maximum(0.0, estimated_energy - self.config.energy_budget_mw)
                    
                    total_loss = task_loss + 0.1 * energy_penalty
                    
                    # Monitor performance
                    metrics = PerformanceMetrics(
                        inference_time=0.001,  # Placeholder
                        energy_mw=float(estimated_energy),
                        memory_usage=0.0,  # Placeholder
                        accuracy=float(1.0 / (1.0 + task_loss))  # Rough accuracy
                    )
                    self.monitor.record_metrics(metrics)
                    
                    return total_loss, {
                        'task_loss': task_loss,
                        'energy_mw': estimated_energy,
                        'energy_penalty': energy_penalty,
                        'total_loss': total_loss,
                        'outputs': outputs,
                        'hidden': hidden
                    }
            
            # Compute gradients with error checking
            (loss_val, aux_data), grads = jax.value_and_grad(loss_fn, has_aux=True)(state['params'])
            
            # Check for gradient explosions
            grad_norm = jnp.sqrt(sum(jnp.sum(g**2) for g in jax.tree_util.tree_leaves(grads)))
            if grad_norm > 10.0:  # Gradient clipping threshold
                grads = jax.tree_util.tree_map(lambda g: g * (10.0 / grad_norm), grads)
            
            # Update parameters
            updates, new_opt_state = self.optimizer.update(grads, state['opt_state'], state['params'])
            new_params = optax.apply_updates(state['params'], updates)
            
            new_state = {
                'params': new_params,
                'opt_state': new_opt_state,
                'step': state['step'] + 1
            }
            
            return new_state, aux_data
            
        except Exception as e:
            self.error_handler.handle_error(e)
            # Return graceful degradation
            return state, {
                'task_loss': jnp.inf,
                'energy_mw': 0.0,
                'energy_penalty': 0.0,
                'total_loss': jnp.inf
            }
    
    def train(self, train_data, targets, epochs=50, batch_size=32):
        """Robust training loop with comprehensive monitoring."""
        print("ğŸ›¡ï¸ Starting robust training with error handling...")
        
        # Initialize training state
        key = jax.random.PRNGKey(42)
        dummy_input = train_data[:1]
        params = self.model.init(key, dummy_input, training=True)
        opt_state = self.optimizer.init(params)
        
        state = {
            'params': params,
            'opt_state': opt_state, 
            'step': 0
        }
        
        dataset_size = train_data.shape[0]
        num_batches = max(1, dataset_size // batch_size)
        
        history = {'loss': [], 'energy': [], 'errors': []}
        consecutive_failures = 0
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            epoch_energy = 0.0
            epoch_errors = 0
            
            try:
                # Shuffle data
                perm = jax.random.permutation(key, dataset_size)
                key, _ = jax.random.split(key)
                
                shuffled_data = train_data[perm]
                shuffled_targets = targets[perm]
                
                for batch_idx in range(num_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min(start_idx + batch_size, dataset_size)
                    
                    batch_data = shuffled_data[start_idx:end_idx]
                    batch_targets = shuffled_targets[start_idx:end_idx]
                    
                    # Safe training step
                    state, metrics = self.safe_train_step(state, (batch_data, batch_targets))
                    
                    if jnp.isfinite(metrics['total_loss']):
                        epoch_loss += float(metrics['total_loss'])
                        epoch_energy += float(metrics['energy_mw'])
                        consecutive_failures = 0
                    else:
                        epoch_errors += 1
                        consecutive_failures += 1
                        
                    # Circuit breaker check
                    if consecutive_failures > 10:
                        print(f"âš ï¸ Too many consecutive failures, stopping training")
                        break
                
                # Calculate averages
                if num_batches > epoch_errors:
                    avg_loss = epoch_loss / (num_batches - epoch_errors)
                    avg_energy = epoch_energy / (num_batches - epoch_errors)
                else:
                    avg_loss = float('inf')
                    avg_energy = 0.0
                
                history['loss'].append(avg_loss)
                history['energy'].append(avg_energy)
                history['errors'].append(epoch_errors)
                
                # Progress reporting
                if epoch % 5 == 0:
                    error_rate = epoch_errors / num_batches * 100
                    print(f"Epoch {epoch:3d}: Loss={avg_loss:.4f}, Energy={avg_energy:.1f}mW, Errors={error_rate:.1f}%")
                
                # Health check
                health = self.monitor.health_check()
                if not health.is_healthy:
                    print(f"âš ï¸ Health check failed: {health.issues}")
                    if 'critical' in str(health.issues).lower():
                        print("ğŸ›‘ Critical issues detected, stopping training")
                        break
                
            except Exception as e:
                print(f"âŒ Epoch {epoch} failed: {e}")
                epoch_errors += 1
                history['errors'].append(epoch_errors)
                
                if consecutive_failures > 5:
                    print("ğŸ›‘ Too many epoch failures, stopping training")
                    break
        
        return {
            'final_params': state['params'],
            'history': history,
            'final_energy_mw': history['energy'][-1] if history['energy'] else 0.0,
            'total_errors': sum(history['errors'])
        }

def main():
    """Main robust training example."""
    print("ğŸ›¡ï¸ Liquid Edge LLN - Robust Training Example")
    print("=" * 55)
    
    # Configuration with safety margins
    config = LiquidConfig(
        input_dim=4,
        hidden_dim=10,  # Slightly smaller for stability
        output_dim=2,
        tau_min=10.0,
        tau_max=100.0,
        use_sparse=False,  # Disable for stability
        energy_budget_mw=100.0,  # Higher budget for stability
        target_fps=50,
        learning_rate=0.0005  # More conservative learning rate
    )
    
    print(f"Model configuration: {config.input_dim}â†’{config.hidden_dim}â†’{config.output_dim}")
    print(f"Safety features: Error handling, circuit breakers, gradient clipping")
    
    # Generate robust test data
    print("\nğŸ“Š Generating robust test data with edge cases...")
    train_data, train_targets = generate_robust_data(500)  # Smaller for testing
    test_data, test_targets = generate_robust_data(100)
    
    print(f"Training data: {train_data.shape}, targets: {train_targets.shape}")
    print(f"Data quality: {jnp.sum(jnp.isnan(train_data))} NaNs, {jnp.sum(jnp.isinf(train_data))} infs")
    
    # Create model
    print("\nğŸ§  Creating robust liquid neural network...")
    model = LiquidNN(config)
    
    # Create robust trainer
    trainer = RobustTrainer(model, config)
    
    # Train with comprehensive error handling
    print("\nğŸš€ Starting robust training...")
    results = trainer.train(
        train_data=train_data,
        targets=train_targets,
        epochs=20,  # Fewer epochs for testing
        batch_size=16
    )
    
    # Results
    final_params = results['final_params']
    training_history = results['history']
    total_errors = results['total_errors']
    
    print(f"\nâœ… Robust training completed!")
    print(f"Final energy: {results['final_energy_mw']:.1f}mW")
    print(f"Total errors handled: {total_errors}")
    if training_history['loss']:
        print(f"Final loss: {training_history['loss'][-1]:.4f}")
        print(f"Loss improvement: {training_history['loss'][0]:.4f} â†’ {training_history['loss'][-1]:.4f}")
    
    # Test robustness
    print("\nğŸ”¬ Testing robustness...")
    try:
        # Test with clean data
        clean_outputs, _ = model.apply(final_params, test_data[:10], training=False)
        print(f"âœ“ Clean data inference: {clean_outputs.shape}")
        
        # Test with corrupted data (NaN injection)
        corrupted_data = test_data[:5].at[0, 0].set(jnp.nan)
        corrupted_outputs, _ = model.apply(final_params, jnp.nan_to_num(corrupted_data), training=False)
        print(f"âœ“ Corrupted data handled: {corrupted_outputs.shape}")
        
        print("âœ… Robustness tests passed!")
        
    except Exception as e:
        print(f"âŒ Robustness test failed: {e}")
    
    # Performance analysis
    print(f"\nğŸ“ˆ Robustness Analysis:")
    if training_history['errors']:
        error_rate = sum(training_history['errors']) / (len(training_history['errors']) * 10) * 100
        print(f"Average error rate: {error_rate:.2f}%")
        print(f"Error resilience: {'Excellent' if error_rate < 5 else 'Good' if error_rate < 15 else 'Needs improvement'}")
    
    print("\nğŸ›¡ï¸ Production-ready robustness features:")
    print("  âœ“ Comprehensive error handling")
    print("  âœ“ Input validation and sanitization") 
    print("  âœ“ Gradient clipping and stability")
    print("  âœ“ Circuit breaker pattern")
    print("  âœ“ Health monitoring and alerts")
    print("  âœ“ Graceful degradation")
    print("  âœ“ Retry mechanisms with backoff")

if __name__ == "__main__":
    main()